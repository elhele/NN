\documentclass[a4paper, 12pt]{article}
\usepackage{titling}
\usepackage{array}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{color} %red, green, blue, yellow, cyan, magenta, black, white
\setlength{\heavyrulewidth}{1.5pt}
\setlength{\abovetopsep}{4pt}
\setlength{\parindent}{0pt}
\graphicspath{{.}}

\usepackage[margin=1in]{geometry}
\definecolor{mygreen}{RGB}{28,172,0} % color values Red, Green, Blue
\definecolor{mylilas}{RGB}{170,55,241}
% Must be after geometry
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\rhead{NN Homework 3}
\lhead{P.Lukin, I. Vishniakou, E. Ovchinnikova}
\cfoot{\thepage}

\setlength{\droptitle}{-5em}

\title{Neural Networks  \\
				- Homework 3 -}
\author{Petr Lukin, Ivan Vishniakou, Evgeniya Ovchinnikova}
\date{Lecture date: 17 October 2016}

\begin{document}

%-------------------------------------------------------------------------------
\lstset{language=Matlab,%
    %basicstyle=\color{red},
    breaklines=true,%
    morekeywords={matlab2tikz},
    keywordstyle=\color{blue},%
    morekeywords=[2]{1}, keywordstyle=[2]{\color{black}},
    identifierstyle=\color{black},%
    stringstyle=\color{mylilas},
    commentstyle=\color{mygreen},%
    showstringspaces=false,%without this there will be a symbol in the places where there is a space
    numbers=left,%
    numberstyle={\tiny \color{black}},% size of the numbers
    numbersep=9pt, % this defines how far the numbers are from the text
    emph=[1]{break},emphstyle=[1]\color{red}, %some words to emphasise
    %emph=[2]{word1,word2}, emphstyle=[2]{style},
}

%-------------------------------------------------------------------------------

\maketitle


\section{Exercises}

\subsection{Exercise 2.1}

The delta rule described in the following equation:\\
$\Delta w_{kj}(n) = \eta e_k(n)x_j(n)$ \\
and Hebb's rule described in:\\
$\Delta w_{kj}(n) = \eta y_k(n)x_j(n)$\\
represent the two different methods of learning. List the features that distinguish these two rules from each other.\\

Solution:\\

\begin{itemize}
\item Learning rule: delta rule is a part of error-correction learning, whereas Hebb's rule is a part of memory-based learning.
\item Adjustment: in delta rule it is based on the difference between desired and actual output (the error) and in Hebb's rule - on correlation, whether two neurons connected by synapse that is being considered are activated simultaneously or not.
\end{itemize}


\subsection{Exercise 2.10}

Formulate the expression for the output $y_j$ of neuron j in the network of Fig. \ref{fig:competiotionNN}, where:\\

$x_i = i^{th}$ input signal,\\
$w_{ji} = $ synaptic weight from input i to neuron j,\\
$c_{kj} = $ weight of lateral connection from neuron k to neuron j,\\
$y_j = \phi(v_j)$.\\



\begin{figure}[h]
  \centering
  \caption{Competitive neural network with feedforward connections.\label{fig:competiotionNN}}
  \includegraphics[width=0.4\textwidth]{competiotionNN}
\end{figure}

What is the condition that would have to be satisfied for neuron j to be the winning neuron?\\

Solution:\\

$$y_j = \phi(\sum_{i=1}^{4} w_{ji}x_i  + \sum_{k=1, k \neq j}^{3} c_{kj}y_k) = \phi(\sum_{i=1}^{4} w_{ji}x_i + \sum_{k=1, k \neq j}^{3} c_{kj}\phi(v_k))$$, where $k_1, k_2$ are numbers from 1 to 3 those are not equal j.\\

Neuron j is a winning neuron if $v_j > v_k$ for all $k \neq j$.

\subsection{Exercise 2.21}

Fill in the details of the level of signal processing next to that described in figure:

\begin{center}
  \includegraphics[scale=1]{21}
\end{center}

\begin{itemize}
\item In this diagram, predictive model block contains and adaptive filter that receives delayed signal and tries to predict current one.
\item Adaptive filter in the model works as linear combiner and can be trained with LMS method.
\item Comparator compares predicted value with plant output to produce error.
\item This error can be used to update weight matrix in model. 
\item The model can be used to continuously predict new values and adapt.
\end{itemize}


\subsection{Exercise 4}

A simple network is given below (from lecture slides). You have to update the weights once using Boltzmann learning for this network. Please do calculations by hand or by using MATLAB or Python.  Use random numbers to initialise the weights. For training use a training set as $[(0, 1), (1, 0)]$ or any training set of your choice.

\begin{figure}[h]
  \centering
  \caption{NN from ex. 4.\label{fig:4NN}}
  \includegraphics[width=0.4\textwidth]{4}
\end{figure}

Because this NN is recurrent and has 5 neurons, weight matrix has size 5x5 with zeros on a main diagonal. The training sample is $s=[1 ,-1,-1,-1,-1]$. Initial weight matrix was filled with $uniform[0,1]$ numbers.

The weight update was done in matlab:

\medskip

\lstinputlisting{Boltzmann.m}


\end{document}
